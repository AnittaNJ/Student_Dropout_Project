{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Business context**\n",
        "Study Group specialises in providing educational services and resources to students and professionals across various fields. The company's primary focus is on enhancing learning experiences through a range of services, including online courses, tutoring, and educational consulting. By leveraging cutting-edge technology and a team of experienced educators, Study Group aims to bridge the gap between traditional learning methods and the evolving needs of today's learners.\n",
        "\n",
        "Study Group serves its university partners by establishing strategic partnerships to enhance the universitiesâ€™ global reach and diversity. It supports the universities in their efforts to attract international students, thereby enriching the cultural and academic landscape of their campuses. It works closely with university faculty and staff to ensure that the universities are prepared and equipped to welcome and support a growing international student body. Its partnership with universities also offers international students a seamless transition into their chosen academic environment. Study Group runs several International Study Centres across the UK and Dublin in partnership with universities with the aim of preparing a pipeline of talented international students from diverse backgrounds for degree study. These centres help international students adapt to the academic, cultural, and social aspects of studying abroad. This is achieved by improving conversational and subject-specific language skills and academic readiness before students progress to a full degree programme at university.\n",
        "\n",
        "Through its comprehensive suite of services, it supports learners and universities at every stage of their educational journey, from high school to postgraduate studies. Its approach is tailored to meet the unique needs of each learner, offering personalised learning paths and flexible scheduling options to accommodate various learning styles and commitments.\n",
        "\n",
        "Study Group's services are designed to be accessible and affordable, making quality education a reality for many individuals. By focusing on the integration of technology and personalised learning, the company aims to empower learners to achieve their full potential and succeed in their academic and professional pursuits. Study Group is at the forefront of transforming how people learn and grow through its dedication to innovation and excellence.\n",
        "Study Group has provided you a course-level data set.\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "## **Objective**\n",
        "By the end of this mini-project, you will have developed the skills and knowledge to apply advanced machine learning techniques to create a predictive model for student dropout. This project will involve comprehensive data exploration, preprocessing, and feature engineering to ensure high-quality input for the models. You will employ and compare multiple predictive algorithms - XGBoost and neural network-based model, to determine the most effective model for predicting student dropout."
      ],
      "metadata": {
        "id": "9j5Bse6izKKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries and Datasets"
      ],
      "metadata": {
        "id": "frCTNdl4KNLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "JOyXMVJKK2E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install keras-tuner"
      ],
      "metadata": {
        "id": "vkuZjzXobpBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score,classification_report,ConfusionMatrixDisplay,roc_curve,auc\n",
        "from datetime import datetime\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import keras_tuner as kt\n",
        "from keras.optimizers import Adam, RMSprop, SGD"
      ],
      "metadata": {
        "id": "Qm7esta9kMkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=(FutureWarning, pd.errors.SettingWithCopyWarning))"
      ],
      "metadata": {
        "id": "kJIee0uyn587"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Dataset"
      ],
      "metadata": {
        "id": "jqhh3vQULPpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "destination = ''\n",
        "\n",
        "# Construct the download URL\n",
        "download_url = ''\n",
        "\n",
        "# Download the file using gdown\n",
        "gdown.download(download_url, destination, quiet=False)"
      ],
      "metadata": {
        "id": "V6cnUUiFMQbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "F6WDreF2Tfay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# view dataframe\n",
        "df = pd.read_csv('CourseLevelDatasetVersion2.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "j2ZDAqC3Ng8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect Data Structure\n",
        "df.info()"
      ],
      "metadata": {
        "id": "6AEM7IicTgps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics for numerical columns.\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "QUgxphnETMmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize categorical columns with frequency counts.\n",
        "df.describe(include='object')"
      ],
      "metadata": {
        "id": "Rl9YIfRSTkJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['ContactHours', 'AttendancePercentage']]"
      ],
      "metadata": {
        "id": "wOt-fxUcFvU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {
        "id": "0YKLb-LBSZFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving a copy of dataset for future purposes\n",
        "df_copy = df.copy()"
      ],
      "metadata": {
        "id": "yHeYyY-xGIwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing columns\n",
        "columns= ['BookingId','BookingType', 'LeadSource', 'DiscountType', 'Nationality', 'HomeCountry', 'HomeState', 'HomeCity', 'PresentCount',\n",
        "          'LateCount', 'AuthorisedAbsenceCount','ArrivedDate','NonCompletionReason', 'TerminationDate', 'CourseFirstIntakeDate', 'CourseStartDate',\n",
        "          'CourseEndDate', 'AcademicYear', 'CourseName', 'LearnerCode', 'ProgressionDegree', 'EligibleToProgress', 'AssessedModules', 'PassedModules',\n",
        "          'FailedModules', 'AttendancePercentage', 'ContactHours']\n",
        "df=df.drop(columns=columns)\n",
        "df"
      ],
      "metadata": {
        "id": "eBLwyNZVPq-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "uYjUbvC7SJGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing rows with missing values\n",
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "DC6yiyfVSkOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for duplicate rows.\n",
        "df.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "pVzxR-Lf3dD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "df"
      ],
      "metadata": {
        "id": "6hQSANHL3h-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "r6FD01Y6YX4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting date of birth column to age."
      ],
      "metadata": {
        "id": "jWKOfGVldKp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DOB column to datetime with the correct format\n",
        "df['DateofBirth'] = pd.to_datetime(df['DateofBirth'], format='%d/%m/%Y')\n",
        "\n",
        "# Function to calculate age\n",
        "def calculate_age(dob):\n",
        "    today = datetime.today()\n",
        "    age = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
        "    return age\n",
        "\n",
        "# Apply the function to the DOB column\n",
        "df['Age'] = df['DateofBirth'].apply(calculate_age)\n",
        "df"
      ],
      "metadata": {
        "id": "t8gJXHTAgZzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing DateofBirth column\n",
        "df = df.drop(columns=['DateofBirth'])\n",
        "df"
      ],
      "metadata": {
        "id": "Ukbzugn_pYlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the numerical columns\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "df"
      ],
      "metadata": {
        "id": "GMjCE9sZjxpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming 'IsFirstIntake' and 'CompletedCourse' columns into binary values."
      ],
      "metadata": {
        "id": "HvUA30m3hDWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking unique values in 'IsFirstIntake' column\n",
        "df['IsFirstIntake'].unique()"
      ],
      "metadata": {
        "id": "F2u7o4lNoKKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming 'IsFirstIntake' column to binary values\n",
        "df['IsFirstIntake'] = df['IsFirstIntake'].map({True: 1, False: 0})\n",
        "df"
      ],
      "metadata": {
        "id": "xCMlKBDIklTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking unique values in 'CompletedCourse' column\n",
        "df['CompletedCourse'].unique()"
      ],
      "metadata": {
        "id": "b6wPvAwhobSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming 'CompletedCourse' column to binary values\n",
        "df['CompletedCourse'] = df['CompletedCourse'].map({'Yes': 1, 'No': 0})\n",
        "df"
      ],
      "metadata": {
        "id": "Ueon1yLqoXMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing one-hot encoding on the categorical columns."
      ],
      "metadata": {
        "id": "mewMqCT6hXec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding of categorical columns\n",
        "df = pd.get_dummies(df, columns=['CentreName','Gender','CourseLevel','ProgressionUniversity'],dtype=int)\n",
        "df"
      ],
      "metadata": {
        "id": "8Bu-iS5FousO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data into features and target\n",
        "features = df.drop('CompletedCourse', axis=1)\n",
        "target = df['CompletedCourse']"
      ],
      "metadata": {
        "id": "l5PuL2jjq22a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for imbalance in the dataset\n",
        "print(\"Original target distribution:\")\n",
        "print(target.value_counts(normalize=True), end=\"\\n\\n\")\n",
        "\n",
        "# Create a list of class names, assuming there are two classes\n",
        "target_names = [\"not completed\" if i == 0 else \"completed\" for i in target.unique()]\n",
        "print(\"Target class names:\")\n",
        "print(target_names)"
      ],
      "metadata": {
        "id": "PBZrHSEkCGDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for the target variable histogram\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.hist(df['CompletedCourse'], bins=[-0.5, 0.5, 1.5], edgecolor='black', alpha=0.7, color='skyblue')\n",
        "plt.xlabel('Completed Course')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Completed Course variable')\n",
        "plt.xticks([0, 1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IkjVhIO0pkHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting boxplots of the input features, grouped by target variable."
      ],
      "metadata": {
        "id": "S6cnaScikLtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot boxplots of the input features, grouped by target variable.\n",
        "for feature in df[numerical_cols].columns:\n",
        "  plt.figure(figsize=(6, 4))\n",
        "  sns.boxplot(x=target, y=feature, data=df)\n",
        "  plt.title(f'Boxplot of {feature} by Completed Course')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "_j3T6d0W2n47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost Model"
      ],
      "metadata": {
        "id": "a2kX53IU59g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial data has been correctly split into training and test set.\n",
        "X_train, X_test, y_train, y_test= train_test_split(features, target, test_size=0.2, random_state=42,stratify=target)"
      ],
      "metadata": {
        "id": "HVHgZ1GD20Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model instantiated and fitted\n",
        "model = xgb.XGBClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "nR85QB8p4rL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model prediction\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "eH23ZyTT6Xof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Indicators"
      ],
      "metadata": {
        "id": "RqtYjbDm5vHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing performance indicators\n",
        "print(\"XG Boost Model Accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cfmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cfmd.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vcs43Nuc5FBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating AUC score\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(\"AUC score:\", auc_score)"
      ],
      "metadata": {
        "id": "0h5dM9Zt0ro5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roc curve for XGBoost model\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.plot(fpr, tpr, label='ROC Curve')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for XGBoost Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3kFer0ox68I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning of XGBoost model."
      ],
      "metadata": {
        "id": "XyEVkMdMou0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tuning of the learning rate, max depth, and number of estimators has been performed.\n",
        "model = xgb.XGBClassifier(random_state=42)\n",
        "param_grid = {\n",
        "  'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "  'max_depth': [3, 5, 7, 9],\n",
        "  'n_estimators': [100, 200, 300, 400]\n",
        "}\n",
        "grid_search = GridSearchCV(\n",
        "  estimator=model,\n",
        "  param_grid=param_grid,\n",
        "  scoring='accuracy',\n",
        "  cv=5,\n",
        "  n_jobs=-1,\n",
        "  verbose=2\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "print('Best parameters found: ', grid_search.best_params_)\n",
        "print('Best accuracy found: ', grid_search.best_score_)"
      ],
      "metadata": {
        "id": "GQ2CcACo7Am1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting model with best parameters from hyper-parameter tuning\n",
        "model = xgb.XGBClassifier(\n",
        "  learning_rate=0.2,\n",
        "  max_depth=3,\n",
        "  n_estimators=100,\n",
        "  random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_2R7XXmc-3WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Indicators"
      ],
      "metadata": {
        "id": "3tNpMpSk-lkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing performance indicators\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred\n",
        "\n",
        "print(\"XG Boost Model Accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cfmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cfmd.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "579jvO_T-4mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating AUC score\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(\"AUC score:\", auc_score)"
      ],
      "metadata": {
        "id": "qBM_ffkS_9uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roc curve for XGBoost model\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.plot(fpr, tpr, label='ROC Curve')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for XGBoost Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fjhNji_YABH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance"
      ],
      "metadata": {
        "id": "DpuPb6sLwkZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to identify important features\n",
        "plt.figure(figsize=(12, 17))\n",
        "feature_importance = pd.Series(model.feature_importances_, index=X_train.columns).sort_values()\n",
        "feature_importance.plot.barh()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dl-kD3dhw1xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shows top 10 important features\n",
        "plt.figure(figsize=(12, 6))\n",
        "feature_importance.iloc[-10:].plot.barh()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vLmhzwWUw2fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding two new features to dataset ('ContactHours', 'AttendancePercentage')"
      ],
      "metadata": {
        "id": "N5Lbv2wuG0wC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two new features ('ContactHours', 'AttendancePercentage') were added back into dataset and trained the XGBoost model with new dataset."
      ],
      "metadata": {
        "id": "n-hpLhN4Aooq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns= ['BookingId','BookingType', 'LeadSource', 'DiscountType', 'Nationality', 'HomeCountry', 'HomeState', 'HomeCity', 'PresentCount',\n",
        "          'LateCount', 'AuthorisedAbsenceCount','ArrivedDate','NonCompletionReason', 'TerminationDate', 'CourseFirstIntakeDate', 'CourseStartDate',\n",
        "          'CourseEndDate', 'AcademicYear', 'CourseName', 'LearnerCode', 'ProgressionDegree', 'EligibleToProgress', 'AssessedModules', 'PassedModules',\n",
        "          'FailedModules']\n",
        "\n",
        "# performing data cleaning and feature engineering steps on new dataset.\n",
        "df = df_copy.drop(columns=columns)\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df['DateofBirth'] = pd.to_datetime(df['DateofBirth'], format='%d/%m/%Y')\n",
        "df['Age'] = df['DateofBirth'].apply(calculate_age)\n",
        "df = df.drop(columns=['DateofBirth'])\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "scaler = StandardScaler()\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "df['IsFirstIntake'] = df['IsFirstIntake'].map({True: 1, False: 0})\n",
        "df['CompletedCourse'] = df['CompletedCourse'].map({'Yes': 1, 'No': 0})\n",
        "df = pd.get_dummies(df, columns=['CentreName', 'Gender','CourseLevel','ProgressionUniversity'],dtype=int)\n",
        "df"
      ],
      "metadata": {
        "id": "QzeIXvhYA70I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost model"
      ],
      "metadata": {
        "id": "0sYJn2EKvCmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data into training and test sets.\n",
        "features = df.drop('CompletedCourse', axis=1)\n",
        "target = df['CompletedCourse']\n",
        "X_train, X_test, y_train, y_test= train_test_split(features, target, test_size=0.2, random_state=42,stratify=target)"
      ],
      "metadata": {
        "id": "vbJEzKBVHlcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model instantiated and fitted\n",
        "model = xgb.XGBClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "yEffgu4wvRWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model prediction\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "88WhGwAqvYwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Indicators"
      ],
      "metadata": {
        "id": "48PPgYQOA7kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing performance indicators\n",
        "print(\"XG Boost Model Accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cfmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cfmd.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H5m73cWFvcKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating AUC score\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(\"AUC score:\", auc_score)"
      ],
      "metadata": {
        "id": "Rvp48PCEAvE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roc curve for XGBoost model\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.plot(fpr, tpr, label='ROC Curve')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for XGBoost Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FFod9bQeA4wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning of XGBoost model"
      ],
      "metadata": {
        "id": "dTd0ftTvvi7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tuning of the learning rate, max depth, and number of estimators has been performed.\n",
        "model = xgb.XGBClassifier(random_state=42)\n",
        "param_grid = {\n",
        "  'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "  'max_depth': [3, 5, 7, 9],\n",
        "  'n_estimators': [100, 200, 300, 400]\n",
        "}\n",
        "grid_search = GridSearchCV(\n",
        "  estimator=model,\n",
        "  param_grid=param_grid,\n",
        "  scoring='accuracy',\n",
        "  cv=5,\n",
        "  n_jobs=-1,\n",
        "  verbose=2\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "print('Best parameters found: ', grid_search.best_params_)\n",
        "print('Best accuracy found: ', grid_search.best_score_)"
      ],
      "metadata": {
        "id": "gHYHI920vvQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting model with best parameters\n",
        "model = xgb.XGBClassifier(\n",
        "  learning_rate=0.1,\n",
        "  max_depth=5,\n",
        "  n_estimators=400,\n",
        "  random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "a559Ah7TvzAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Indicators"
      ],
      "metadata": {
        "id": "Iw6qKU3vDgqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing performance indicators\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred\n",
        "\n",
        "print(\"XG Boost Model Accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cfmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cfmd.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "25Y0fJ6Rv6a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(\"AUC score:\", auc_score)"
      ],
      "metadata": {
        "id": "ALqnrpIPH_bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roc curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.plot(fpr, tpr, label='ROC Curve')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for XGBoost Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zFy8EDIeIC2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance"
      ],
      "metadata": {
        "id": "P9t1efj1wQli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance plotted after adding two new features back to dataset.\n",
        "plt.figure(figsize=(12, 20))\n",
        "feature_importance = pd.Series(model.feature_importances_, index=X_train.columns).sort_values()\n",
        "\n",
        "feature_importance.plot.barh()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FYQDmVwnKLrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "feature_importance.iloc[-10:].plot.barh()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y7okSLsFKswW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Model"
      ],
      "metadata": {
        "id": "mbXf3arCM6LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is preprocessed for neural network model."
      ],
      "metadata": {
        "id": "PZMnR-5I4mLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns= ['BookingId','BookingType', 'LeadSource', 'DiscountType', 'Nationality', 'HomeCountry', 'HomeState', 'HomeCity', 'PresentCount',\n",
        "          'LateCount', 'AuthorisedAbsenceCount','ArrivedDate','NonCompletionReason', 'TerminationDate', 'CourseFirstIntakeDate', 'CourseStartDate',\n",
        "          'CourseEndDate', 'AcademicYear', 'CourseName', 'LearnerCode', 'ProgressionDegree', 'EligibleToProgress', 'AssessedModules', 'PassedModules',\n",
        "          'FailedModules','AttendancePercentage', 'ContactHours']\n",
        "\n",
        "# performing data cleaning and feature engineering steps on new dataset.\n",
        "df=df_copy.drop(columns=columns)\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df['DateofBirth'] = pd.to_datetime(df['DateofBirth'], format='%d/%m/%Y')\n",
        "df['Age'] = df['DateofBirth'].apply(calculate_age)\n",
        "df = df.drop(columns=['DateofBirth'])\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "scaler = StandardScaler()\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "df['IsFirstIntake'] = df['IsFirstIntake'].map({True: 1, False: 0})\n",
        "df['CompletedCourse'] = df['CompletedCourse'].map({'Yes': 1, 'No': 0})\n",
        "df = pd.get_dummies(df, columns=['CentreName', 'Gender','CourseLevel','ProgressionUniversity'],dtype=int)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "ESXh2aCxNuFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data into training and test sets.\n",
        "features = df.drop('CompletedCourse', axis=1)\n",
        "target = df['CompletedCourse']\n",
        "X_train, X_test, y_train, y_test= train_test_split(features, target, test_size=0.2, random_state=42,stratify=target)"
      ],
      "metadata": {
        "id": "HV0Bpz37NtHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "  Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "  Dropout(0.2),\n",
        "  Dense(32, activation='relu'),\n",
        "  Dropout(0.2),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-zlaFM1NN9QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 5\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "8ZsNdM4XQmtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking test loss and test accuracy of best model.\n",
        "model = load_model('best_model.keras')\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('test_loss', test_loss)\n",
        "print('test_accuracy', test_accuracy)"
      ],
      "metadata": {
        "id": "qiuIF1kLPU40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Indicators"
      ],
      "metadata": {
        "id": "iMHzJFeynusD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# For binary classification, assume a threshold of 0.5\n",
        "y_pred = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"Neural Network: \", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cfmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cfmd.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PV6XC9iE3di7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities for the test data\n",
        "ptest = model.predict(X_test)\n",
        "\n",
        "# Calculate the AUC score\n",
        "auc = roc_auc_score(y_test, ptest)\n",
        "\n",
        "# Print the AUC score\n",
        "print(f\"AUC score: {auc}\")\n",
        "\n",
        "# Plot the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, ptest)\n",
        "\n",
        "plt.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
        "plt.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"Random Classifier\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kgIa9lx2H_hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract loss values\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Extract accuracy values\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
        "\n",
        "# Plot the loss curves\n",
        "axs[0].plot( train_loss, label='Training Loss')\n",
        "axs[0].plot( val_loss, label='Validation Loss')\n",
        "axs[0].set_title('Training and Validation Loss')\n",
        "axs[0].set_xlabel('Epochs')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].legend()\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, plot the accuracy curves\n",
        "axs[1].plot( train_accuracy, label='Training Accuracy')\n",
        "axs[1].plot(val_accuracy, label='Validation Accuracy')\n",
        "axs[1].set_title('Training and Validation Accuracy')\n",
        "axs[1].set_xlabel('Epochs')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LQSishRsGW_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning of Neural Network using looping."
      ],
      "metadata": {
        "id": "fpNTewcE7hFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the hyperparameter with different values for tuning the model."
      ],
      "metadata": {
        "id": "GB1M0-l-pkVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of neurons.\n",
        "num_neurons = [(128,64),(64,32),(32,16),(128,32),(64,16)]\n",
        "\n",
        "# Define the optimizers.\n",
        "optimizers = ['adam', 'rmsprop','sgd']\n",
        "\n",
        "# Define the activation function.\n",
        "activation_fn = ['relu', 'tanh', 'leaky_relu']"
      ],
      "metadata": {
        "id": "J4yPEczDXGhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a function called 'create_model' to call model definition for future purposes.\n",
        "def create_model(num_neurons, optimizers, activation_fn):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_neurons[0], activation=activation_fn, input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_neurons[1], activation=activation_fn))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    if optimizers == 'adam':\n",
        "        opt = Adam()\n",
        "    elif optimizers == 'rmsprop':\n",
        "        opt = RMSprop()\n",
        "    elif optimizers == 'sgd':\n",
        "        opt = SGD()\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xHWjGKTojC9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models with different number of neurons, optimizers and activation function.\n",
        "results = {}\n",
        "patience = 5\n",
        "best_val_accuracy = 0\n",
        "best_hyperparams = None\n",
        "\n",
        "for neurons in num_neurons:\n",
        "    for opt in optimizers:\n",
        "        for activation in activation_fn:\n",
        "            print(f\"Training with num_neurons={neurons}, optimizer={opt}, activation_fn={activation}\")\n",
        "            model = create_model(num_neurons=neurons, optimizers=opt, activation_fn=activation)\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "            model_checkpoint = ModelCheckpoint('temp_model.keras', monitor='val_loss', save_best_only=True)\n",
        "            history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "            # Load the best model from the current training\n",
        "            temp_model = load_model('temp_model.keras')\n",
        "            val_accuracy = history.history['val_accuracy'][-1]\n",
        "            results[(neurons, opt, activation)] = val_accuracy\n",
        "            print(f\"Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "            # Check if the current model is the best one\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_hyperparams = (neurons, opt, activation)\n"
      ],
      "metadata": {
        "id": "KsMBK1oVh1ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best hyperparameters: {best_hyperparams}\")\n",
        "print(f\"Best validation accuracy: {best_val_accuracy}\")"
      ],
      "metadata": {
        "id": "t75CxNAGNHfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting model with best parameters from hyper-parameter tuning\n",
        "model = Sequential([\n",
        "  Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "  Dropout(0.2),\n",
        "  Dense(32, activation='relu'),\n",
        "  Dropout(0.2),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Jks-YfqaAUBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 5\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "sS9Cb10DAL4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best model\n",
        "model = load_model('best_model.keras')\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('test_loss', test_loss)\n",
        "print('test_accuracy', test_accuracy)"
      ],
      "metadata": {
        "id": "FB0WP0nIqgKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Indicators."
      ],
      "metadata": {
        "id": "sZMkhaJyrI-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# For binary classification, assume a threshold of 0.5\n",
        "y_pred = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"Neural Network: \", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cfmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cfmd.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hhQNL9xZtvvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities for the test data\n",
        "ptest = model.predict(X_test)\n",
        "\n",
        "# Calculate the AUC score\n",
        "auc = roc_auc_score(y_test, ptest)\n",
        "\n",
        "# Print the AUC score\n",
        "print(f\"AUC score: {auc}\")\n",
        "\n",
        "# Plot the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, ptest)\n",
        "\n",
        "plt.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
        "plt.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"Random Classifier\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_0JkfQUNIqbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract loss values\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Extract accuracy values (optional, for completeness)\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
        "\n",
        "# Plot the loss curves\n",
        "axs[0].plot( train_loss, label='Training Loss')\n",
        "axs[0].plot( val_loss, label='Validation Loss')\n",
        "axs[0].set_title('Training and Validation Loss')\n",
        "axs[0].set_xlabel('Epochs')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].legend()\n",
        "\n",
        "#plot the accuracy curves\n",
        "axs[1].plot( train_accuracy, label='Training Accuracy')\n",
        "axs[1].plot(val_accuracy, label='Validation Accuracy')\n",
        "axs[1].set_title('Training and Validation Accuracy')\n",
        "axs[1].set_xlabel('Epochs')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e-FsoNyduHeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding two new features to dataset ('ContactHours', 'AttendancePercentage')"
      ],
      "metadata": {
        "id": "-QG31JUlv2af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two new features ('ContactHours', 'AttendancePercentage') were added back into dataset and to train the Neural Network model with new dataset."
      ],
      "metadata": {
        "id": "i2NKhGXRsfb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns= ['BookingId','BookingType', 'LeadSource', 'DiscountType', 'Nationality', 'HomeCountry', 'HomeState', 'HomeCity', 'PresentCount',\n",
        "          'LateCount', 'AuthorisedAbsenceCount','ArrivedDate','NonCompletionReason', 'TerminationDate', 'CourseFirstIntakeDate', 'CourseStartDate',\n",
        "          'CourseEndDate', 'AcademicYear', 'CourseName', 'LearnerCode', 'ProgressionDegree', 'EligibleToProgress', 'AssessedModules', 'PassedModules',\n",
        "          'FailedModules']\n",
        "\n",
        "# performing data cleaning and feature engineering steps on new dataset.\n",
        "df = df_copy.drop(columns=columns)\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df['DateofBirth'] = pd.to_datetime(df['DateofBirth'], format='%d/%m/%Y')\n",
        "df['Age'] = df['DateofBirth'].apply(calculate_age)\n",
        "df = df.drop(columns=['DateofBirth'])\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "scaler = StandardScaler()\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "df['IsFirstIntake'] = df['IsFirstIntake'].map({True: 1, False: 0})\n",
        "df['CompletedCourse'] = df['CompletedCourse'].map({'Yes': 1, 'No': 0})\n",
        "df = pd.get_dummies(df, columns=['CentreName', 'Gender','CourseLevel','ProgressionUniversity'],dtype=int)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "iQIdTQURvoGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data into training and test sets.\n",
        "features = df.drop('CompletedCourse', axis=1)\n",
        "target = df['CompletedCourse']\n",
        "X_train, X_test, y_train, y_test= train_test_split(features, target, test_size=0.2, random_state=42,stratify=target)"
      ],
      "metadata": {
        "id": "CgVL82C6v6Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "  Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "  Dropout(0.2),\n",
        "  Dense(32, activation='relu'),\n",
        "  Dropout(0.2),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "O3vK7xdwv_DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 5\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "vkPKfggwwNle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best model\n",
        "model = load_model('best_model.keras')\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('test_loss', test_loss)\n",
        "print('test_accuracy', test_accuracy)"
      ],
      "metadata": {
        "id": "l4cDXppnwYLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Indicators"
      ],
      "metadata": {
        "id": "XIW9-J-is_RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# For binary classification, assume a threshold of 0.5\n",
        "y_pred = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"Neural Network: \", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cfmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cfmd.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UiHoURmjwhPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities for the test data\n",
        "ptest = model.predict(X_test)\n",
        "\n",
        "# Calculate the AUC score\n",
        "auc = roc_auc_score(y_test, ptest)\n",
        "\n",
        "# Print the AUC score\n",
        "print(f\"AUC score: {auc}\")\n",
        "\n",
        "# Plot the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, ptest)\n",
        "\n",
        "plt.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
        "plt.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"Random Classifier\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OTPDXaC9JaGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract loss values\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Extract accuracy values (optional, for completeness)\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
        "\n",
        "# Plot the loss curves\n",
        "axs[0].plot( train_loss, label='Training Loss')\n",
        "axs[0].plot( val_loss, label='Validation Loss')\n",
        "axs[0].set_title('Training and Validation Loss')\n",
        "axs[0].set_xlabel('Epochs')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].legend()\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, plot the accuracy curves\n",
        "axs[1].plot( train_accuracy, label='Training Accuracy')\n",
        "axs[1].plot(val_accuracy, label='Validation Accuracy')\n",
        "axs[1].set_title('Training and Validation Accuracy')\n",
        "axs[1].set_xlabel('Epochs')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EapU91xhwlmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning of NN using looping."
      ],
      "metadata": {
        "id": "gYoG3nhsw8co"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the hyperparameter with different values for tuning the model."
      ],
      "metadata": {
        "id": "SlR-N0VMtxSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of neurons.\n",
        "num_neurons = [(128,64),(64,32),(32,16),(128,32),(64,16)]\n",
        "\n",
        "# Define the optimizers.\n",
        "optimizers = ['adam', 'rmsprop','sgd']\n",
        "\n",
        "# Define the activation function.\n",
        "activation_fn = ['relu', 'tanh', 'leaky_relu']"
      ],
      "metadata": {
        "id": "DQargL65w_fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_neurons, optimizers, activation_fn):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_neurons[0], activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_neurons[1], activation=activation_fn))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    if optimizers == 'adam':\n",
        "        opt = Adam()\n",
        "    elif optimizers == 'rmsprop':\n",
        "        opt = RMSprop()\n",
        "    elif optimizers == 'sgd':\n",
        "        opt = SGD()\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "lYO1ZytyxF-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models with different number of neurons, optimizers and activation function.\n",
        "results = {}\n",
        "patience = 5\n",
        "best_val_accuracy = 0\n",
        "best_hyperparams = None\n",
        "\n",
        "for neurons in num_neurons:\n",
        "    for opt in optimizers:\n",
        "        for activation in activation_fn:\n",
        "            print(f\"Training with num_neurons={neurons}, optimizer={opt}, activation_fn={activation}\")\n",
        "            model = create_model(num_neurons=neurons, optimizers=opt, activation_fn=activation)\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "            model_checkpoint = ModelCheckpoint('temp_model.keras', monitor='val_loss', save_best_only=True)\n",
        "            history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "            # Load the best model from the current training\n",
        "            temp_model = load_model('temp_model.keras')\n",
        "            val_accuracy = history.history['val_accuracy'][-1]\n",
        "            results[(neurons, opt, activation)] = val_accuracy\n",
        "            print(f\"Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "            # Check if the current model is the best one\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_hyperparams = (neurons, opt, activation)\n"
      ],
      "metadata": {
        "id": "cDmv8j68xJz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best hyperparameters: {best_hyperparams}\")\n",
        "print(f\"Best validation accuracy: {best_val_accuracy}\")"
      ],
      "metadata": {
        "id": "6OR-ZUpKMTtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting model with best parameters from hyper-parameter tuning\n",
        "model = Sequential([\n",
        "  Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "  Dropout(0.2),\n",
        "  Dense(64, activation='tanh'),\n",
        "  Dropout(0.2),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Izf_U0fmxKwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 5\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "dNp9WZ8sK9qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('best_model.keras')\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('test_loss', test_loss)\n",
        "print('test_accuracy', test_accuracy)"
      ],
      "metadata": {
        "id": "4kFDIAPfLCDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Indicators"
      ],
      "metadata": {
        "id": "x54oNeAkuOHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# For binary classification, assume a threshold of 0.5\n",
        "y_pred = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"Neural Network: \", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cfmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cfmd.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n6Sd3d0lxR2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities for the test data\n",
        "ptest = model.predict(X_test)\n",
        "\n",
        "# Calculate the AUC score\n",
        "auc = roc_auc_score(y_test, ptest)\n",
        "\n",
        "# Print the AUC score\n",
        "print(f\"AUC score: {auc}\")\n",
        "\n",
        "# Plot the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, ptest)\n",
        "\n",
        "plt.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
        "plt.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"Random Classifier\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7WmPvHycLK7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract loss values\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Extract accuracy values (optional, for completeness)\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
        "\n",
        "# Plot the loss curves\n",
        "axs[0].plot( train_loss, label='Training Loss')\n",
        "axs[0].plot( val_loss, label='Validation Loss')\n",
        "axs[0].set_title('Training and Validation Loss')\n",
        "axs[0].set_xlabel('Epochs')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].legend()\n",
        "\n",
        "\n",
        "\n",
        "#plot the accuracy curves\n",
        "axs[1].plot( train_accuracy, label='Training Accuracy')\n",
        "axs[1].plot(val_accuracy, label='Validation Accuracy')\n",
        "axs[1].set_title('Training and Validation Accuracy')\n",
        "axs[1].set_xlabel('Epochs')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yBRclP2_xa-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}